# markets/cn/downloader.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import random
import sqlite3
import subprocess
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import pandas as pd
from tqdm import tqdm

from .cn_config import (
    default_db_path,
    rolling_trading_days,
    fallback_rolling_cal_days,
    log,
    fix_sector_flag,
    db_vacuum,
    sync_sw_industry_flag,
    sw_only_missing,
    sw_max_industries,
    sw_sector_level,
    sw_sync_timeout,
)
from .cn_db import init_db
from .cn_calendar import infer_window_by_trading_days
from .cn_stock_list import get_cn_stock_list
from .cn_market import is_main, is_chinext, is_star
from .cn_prices import (
    download_one,
    download_batch,
    insert_prices,
    write_final_errors,
    batch_size,
    batch_sleep_sec,
    fallback_single_enabled,
    sleep_between,  # single fallback ç¯€å¥ç”¨
)

# -----------------------------------------------------------------------------
# sector clean (ä¿ç•™ä½ åŸæœ¬åŠŸèƒ½)
# -----------------------------------------------------------------------------
BAD_SECTOR_SQL = """
UPDATE stock_info
SET sector='æœªåˆ†é¡'
WHERE sector IS NULL
   OR TRIM(sector)=''
   OR sector IN ('A-Share','â€”','-','--','ï¼','â€“')
"""


def fix_sector_missing(db_path: str) -> int:
    conn = sqlite3.connect(db_path)
    try:
        cur = conn.execute(BAD_SECTOR_SQL)
        conn.commit()
        return int(cur.rowcount or 0)
    finally:
        conn.close()


def _chunk_list(xs: List[str], n: int) -> List[List[str]]:
    if n <= 0:
        n = 1
    return [xs[i : i + n] for i in range(0, len(xs), n)]


# -----------------------------------------------------------------------------
# SW cache sync (FAST PATH)
# -----------------------------------------------------------------------------
def _sw_cache_path() -> str:
    # é è¨­æ‰¾ markets/cn/cn_sw_merged_cache.json
    return os.getenv(
        "CN_SW_CACHE_PATH",
        os.path.join(os.path.dirname(__file__), "cn_sw_merged_cache.json"),
    )


def _sync_sw_from_cache(db_path: str) -> bool:
    """
    å„ªå…ˆèµ°å¿«è·¯ï¼šæŠŠ cn_sw_merged_cache.json å¯«å› stock_infoï¼ˆåªè£œç¼ºå¤±ï¼‰
    æˆåŠŸå› Trueï¼›å¤±æ•—/æª”ä¸å­˜åœ¨å› Falseï¼ˆè®“å¤–å±¤ fallback åˆ°æ…¢ç‰ˆï¼‰ã€‚
    """
    cache_path = _sw_cache_path()
    if not os.path.exists(cache_path):
        log(f"ğŸ·ï¸ SW cache not found, skip fast sync: {cache_path}")
        return False

    script_path = os.path.join(os.path.dirname(__file__), "sync_sw_cache_to_db.py")
    if not os.path.exists(script_path):
        log(f"ğŸ·ï¸ SW cache sync script missing, skip fast sync: {script_path}")
        return False

    timeout_s = sw_sync_timeout()
    try:
        log(f"ğŸ·ï¸ SW cache sync (FAST) ... only-missing | cache={os.path.basename(cache_path)}")
        cmd = ["python", script_path, "--db", db_path, "--cache", cache_path, "--only-missing"]
        subprocess.run(cmd, check=True, timeout=timeout_s)
        log("âœ… SW cache sync done.")
        return True
    except subprocess.TimeoutExpired:
        log("âš ï¸ SW cache sync timeout (fallback to slow sync if enabled).")
        return False
    except Exception as e:
        log(f"âš ï¸ SW cache sync failed (fallback to slow sync if enabled): {e}")
        return False


def run_sync(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    *,
    refresh_list: bool = True,
    sample_n: int = 0,
    sample_mode: str = "mixed",
    symbols: Optional[List[str]] = None,
) -> Dict[str, Any]:
    db_path = default_db_path()
    init_db(db_path)

    if not end_date:
        end_date = datetime.now().strftime("%Y-%m-%d")

    n_days = rolling_trading_days()
    start_td, end_incl, end_excl = infer_window_by_trading_days(end_date, n_days)

    if (not start_date) and start_td and end_incl and end_excl:
        start_date = start_td
        end_date = end_incl
        end_excl_date = end_excl
        mode = "trading_days"
        log(
            f"ğŸ“… Trading-day window OK | last {n_days} trading days | {start_date} ~ {end_date} (end_excl={end_excl_date})"
        )
    else:
        if not start_date:
            start_date = (datetime.now() - timedelta(days=fallback_rolling_cal_days())).strftime("%Y-%m-%d")
        end_excl_date = (pd.to_datetime(end_date) + timedelta(days=1)).strftime("%Y-%m-%d")
        mode = "cal_days"
        log(f"âš ï¸ Trading-day window unavailable; fallback to cal-days | {start_date} ~ {end_date} (end_excl={end_excl_date})")

    log(f"ğŸ“¦ CN DB = {db_path}")
    log(f"ğŸš€ CN run_sync | window: {start_date} ~ {end_date} | refresh_list={refresh_list}")

    # 1) build list
    if symbols:
        items = [(s.strip(), "Unknown") for s in symbols if str(s).strip()]
        log(f"ğŸ§ª æŒ‡å®š symbols æ¨¡å¼ï¼š{len(items)} æª”")
    else:
        items = get_cn_stock_list(db_path, refresh_list=refresh_list)

    if not items:
        return {"success": 0, "total": 0, "failed": 0, "has_changed": False, "db_path": db_path}

    # 2) optional sampling (debug)
    if sample_n and sample_n > 0:
        mode_s = (sample_mode or "mixed").lower().strip()

        main_items = [(s, n) for s, n in items if is_main(s)]
        chinext_items = [(s, n) for s, n in items if is_chinext(s)]
        star_items = [(s, n) for s, n in items if is_star(s)]

        if mode_s == "main":
            items = random.sample(main_items, min(sample_n, len(main_items)))
        elif mode_s == "chinext":
            items = random.sample(chinext_items, min(sample_n, len(chinext_items)))
        elif mode_s == "star":
            items = random.sample(star_items, min(sample_n, len(star_items)))
        else:
            k = max(1, sample_n // 3)
            pick_main = random.sample(main_items, min(k, len(main_items)))
            pick_chi = random.sample(chinext_items, min(k, len(chinext_items)))
            pick_star = random.sample(star_items, min(k, len(star_items)))
            items = pick_main + pick_chi + pick_star

        log(f"ğŸ§ª SAMPLE MODE: {mode_s} | symbols={len(items)}")

    # 3) rolling delete (avoid DB growing)
    conn = sqlite3.connect(db_path, timeout=120)
    try:
        conn.execute("DELETE FROM stock_prices WHERE date >= ?", (start_date,))
        conn.commit()
    finally:
        conn.close()

    # 4) batch download & upsert
    success = 0
    failed = 0

    name_map: Dict[str, str] = {s: (n or "Unknown") for s, n in items if s}
    all_syms: List[str] = [s for s, _ in items if s]

    bs = max(1, int(batch_size()))
    bs_sleep = float(batch_sleep_sec())
    do_fallback = bool(fallback_single_enabled())

    log(f"ğŸ§© CN batch download enabled | batch_size={bs} | batch_sleep={bs_sleep} | fallback_single={do_fallback}")

    final_failed: Dict[str, str] = {}

    conn = sqlite3.connect(db_path, timeout=120)
    try:
        batches = _chunk_list(all_syms, bs)
        pbar = tqdm(batches, desc="CNåŒæ­¥(batch)", unit="æ‰¹")

        for tickers in pbar:
            # (A) batch download
            df_long, failed_tickers, berr = download_batch(tickers, start_date, end_excl_date)

            # (A1) insert success rows
            if df_long is not None and (not df_long.empty):
                try:
                    ok_syms = set(df_long["symbol"].astype(str).unique().tolist())
                except Exception:
                    ok_syms = set()
                insert_prices(conn, df_long)
                if ok_syms:
                    success += len(ok_syms)

            # (A2) batch-level failure info
            if failed_tickers:
                if berr:
                    for sym in failed_tickers:
                        final_failed[sym] = berr
                else:
                    for sym in failed_tickers:
                        final_failed.setdefault(sym, "batch_failed")

                # (B) fallback single download (optional)
                if do_fallback:
                    for sym in failed_tickers:
                        df_one, err_one = download_one(sym, start_date, end_excl_date)
                        if df_one is not None and (not df_one.empty):
                            insert_prices(conn, df_one)
                            success += 1
                            final_failed.pop(sym, None)
                        else:
                            failed += 1
                            if err_one:
                                final_failed[sym] = err_one
                            else:
                                final_failed.setdefault(sym, "empty")

                        sleep_between()
                else:
                    failed += len(failed_tickers)

            if bs_sleep > 0:
                time.sleep(bs_sleep)

        conn.commit()

        # âœ… åªå¯«ã€Œæœ€çµ‚ä»å¤±æ•—ã€çš„ tickerï¼ˆä¹¾æ·¨ã€ä¸é‡è¤‡ï¼‰
        write_final_errors(conn, final_failed, name_map, start_date, end_date)

        # å…ˆæŠŠ sector ç©º/å£å€¼æ•´ç†æˆã€Œæœªåˆ†é¡ã€
        if fix_sector_flag():
            affected = fix_sector_missing(db_path)
            log(f"ğŸ·ï¸ sector ç¼ºå¤±/å£å€¼ â†’ æœªåˆ†é¡ï¼š{affected} ç­†")

        # âœ… SW industryï¼šFAST PATH å„ªå…ˆç”¨ cacheï¼›å¤±æ•—æ‰ fallback æ…¢ç‰ˆï¼ˆä¾ flagï¼‰
        did_fast = _sync_sw_from_cache(db_path)

        if (not did_fast) and sync_sw_industry_flag():
            # fallback slow sync
            try:
                script_path = os.path.join(os.path.dirname(__file__), "sw_industry_sync.py")
                timeout_s = sw_sync_timeout()
                only_missing = sw_only_missing()
                max_ind = sw_max_industries()
                level = sw_sector_level()

                cmd = ["python", script_path, "--db", db_path, "--sector-level", level]
                if only_missing:
                    cmd.append("--only-missing")
                if max_ind:
                    cmd += ["--max-industries", max_ind]

                log(f"ğŸ·ï¸ SW industry sync (SLOW) ... ({'only-missing' if only_missing else 'full'}) | sector_level={level}")
                subprocess.run(cmd, check=True, timeout=timeout_s)
                log("âœ… SW industry sync done.")
            except subprocess.TimeoutExpired:
                log("âš ï¸ SW industry sync timeout (continue).")
            except Exception as e:
                log(f"âš ï¸ SW industry sync failed (continue): {e}")
        else:
            if did_fast:
                log("ğŸš€ SW industry: used FAST cache sync; skip slow sync.")
            else:
                log("ğŸ·ï¸ SW industry: slow sync disabled by flag; skip.")

        if db_vacuum():
            log("ğŸ§¹ VACUUM...")
            conn.execute("VACUUM")
            conn.commit()

        total = conn.execute("SELECT COUNT(DISTINCT symbol) FROM stock_info").fetchone()[0]
    finally:
        conn.close()

    if final_failed:
        failed = len(final_failed)

    log(f"ğŸ“Š CN åŒæ­¥å®Œæˆ | æˆåŠŸ:{success} å¤±æ•—:{failed} / {len(all_syms)}")
    return {
        "success": success,
        "total": int(total or 0),
        "failed": failed,
        "has_changed": success > 0,
        "window": {"start": start_date, "end": end_date, "end_excl": end_excl_date, "mode": mode},
        "db_path": db_path,
        "calendar": {
            "ticker": os.getenv("CN_CALENDAR_TICKER", "000001.SS"),
            "n_trading_days": int(n_days),
            "lookback_cal_days": int(os.getenv("CN_CAL_LOOKBACK_CAL_DAYS", "180")),
        },
        "batch": {
            "batch_size": int(bs),
            "batch_sleep_sec": float(bs_sleep),
            "fallback_single": bool(do_fallback),
            "final_failed": int(len(final_failed)),
        },
    }


def run_intraday(*, slot: str, asof: str, ymd: str) -> Dict[str, Any]:
    # âœ… ä¿®æ­£ ImportErrorï¼šsnapshot_builder å°å¤–æ˜¯ run_intradayï¼Œä¸æ˜¯ build_snapshot_payload
    from .snapshot_builder import run_intraday as _run_intraday
    return _run_intraday(slot=slot, asof=asof, ymd=ymd)
